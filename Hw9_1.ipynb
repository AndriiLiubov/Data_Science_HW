{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "#Download dataset\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "obowRMQ138g9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalization\n",
        "\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0"
      ],
      "metadata": {
        "id": "4wQuF8bm4emW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#One-hot encoding\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes=10)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes=10)"
      ],
      "metadata": {
        "id": "tOCVJaE74uD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import regularizers"
      ],
      "metadata": {
        "id": "2liMLmjc4j-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# activation in [\"relu\", \"sigmoid\", \"tanh\", \"softmax\", \"leaky_relu\"]\n",
        "# initializer in [\"random_normal\", \"glorot_normal\", \"he_normal\"]\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(512, activation='relu'),\n",
        "    keras.layers.Dropout(0.3),\n",
        "    keras.layers.Dense(32, activation='relu'),\n",
        "    # keras.layers.Dense(64, activation='relu', kernel_initializer=\"he_normal\", kernel_regularizer=regularizers.l2()),\n",
        "    # keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "QMXoCXVW6Fue"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer in [\"sgd\", \"adagrad\", \"rmsprop\", \"adam\"]\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adagrad',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "AbTWMFLy6I4Z"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, y_train, epochs=33, batch_size=300, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCWVVUGD6JWO",
        "outputId": "7e2010d9-dda7-4d5d-d574-ca8af820dcb7"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/33\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - accuracy: 0.9577 - loss: 0.1116 - val_accuracy: 0.9023 - val_loss: 0.3758\n",
            "Epoch 2/33\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.9642 - loss: 0.0932 - val_accuracy: 0.9029 - val_loss: 0.3705\n",
            "Epoch 3/33\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 27ms/step - accuracy: 0.9661 - loss: 0.0902 - val_accuracy: 0.9038 - val_loss: 0.3679\n",
            "Epoch 4/33\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.9680 - loss: 0.0866 - val_accuracy: 0.9045 - val_loss: 0.3670\n",
            "Epoch 5/33\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - accuracy: 0.9693 - loss: 0.0812 - val_accuracy: 0.9046 - val_loss: 0.3669\n",
            "Epoch 6/33\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 30ms/step - accuracy: 0.9691 - loss: 0.0812 - val_accuracy: 0.9047 - val_loss: 0.3665\n",
            "Epoch 7/33\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.9708 - loss: 0.0781 - val_accuracy: 0.9050 - val_loss: 0.3668\n",
            "Epoch 8/33\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - accuracy: 0.9718 - loss: 0.0768 - val_accuracy: 0.9050 - val_loss: 0.3675\n",
            "Epoch 9/33\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 29ms/step - accuracy: 0.9704 - loss: 0.0786 - val_accuracy: 0.9053 - val_loss: 0.3680\n",
            "Epoch 10/33\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.9724 - loss: 0.0774 - val_accuracy: 0.9059 - val_loss: 0.3680\n",
            "Epoch 11/33\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - accuracy: 0.9709 - loss: 0.0783 - val_accuracy: 0.9059 - val_loss: 0.3682\n",
            "Epoch 12/33\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.9716 - loss: 0.0774 - val_accuracy: 0.9057 - val_loss: 0.3683\n",
            "Epoch 13/33\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.9715 - loss: 0.0769 - val_accuracy: 0.9054 - val_loss: 0.3684\n",
            "Epoch 14/33\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.9727 - loss: 0.0737 - val_accuracy: 0.9057 - val_loss: 0.3687\n",
            "Epoch 15/33\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 30ms/step - accuracy: 0.9720 - loss: 0.0742 - val_accuracy: 0.9053 - val_loss: 0.3687\n",
            "Epoch 16/33\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.9734 - loss: 0.0719 - val_accuracy: 0.9053 - val_loss: 0.3698\n",
            "Epoch 17/33\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.9745 - loss: 0.0707 - val_accuracy: 0.9056 - val_loss: 0.3702\n",
            "Epoch 18/33\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 28ms/step - accuracy: 0.9733 - loss: 0.0729 - val_accuracy: 0.9059 - val_loss: 0.3701\n",
            "Epoch 19/33\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.9733 - loss: 0.0720 - val_accuracy: 0.9058 - val_loss: 0.3704\n",
            "Epoch 20/33\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - accuracy: 0.9743 - loss: 0.0717 - val_accuracy: 0.9064 - val_loss: 0.3704\n",
            "Epoch 21/33\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 29ms/step - accuracy: 0.9740 - loss: 0.0710 - val_accuracy: 0.9067 - val_loss: 0.3708\n",
            "Epoch 22/33\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.9731 - loss: 0.0704 - val_accuracy: 0.9066 - val_loss: 0.3715\n",
            "Epoch 23/33\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.9734 - loss: 0.0730 - val_accuracy: 0.9068 - val_loss: 0.3718\n",
            "Epoch 24/33\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 28ms/step - accuracy: 0.9730 - loss: 0.0722 - val_accuracy: 0.9075 - val_loss: 0.3719\n",
            "Epoch 25/33\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.9738 - loss: 0.0718 - val_accuracy: 0.9078 - val_loss: 0.3724\n",
            "Epoch 26/33\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.9736 - loss: 0.0717 - val_accuracy: 0.9077 - val_loss: 0.3722\n",
            "Epoch 27/33\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - accuracy: 0.9739 - loss: 0.0702 - val_accuracy: 0.9072 - val_loss: 0.3724\n",
            "Epoch 28/33\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 25ms/step - accuracy: 0.9756 - loss: 0.0709 - val_accuracy: 0.9072 - val_loss: 0.3725\n",
            "Epoch 29/33\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - accuracy: 0.9730 - loss: 0.0710 - val_accuracy: 0.9068 - val_loss: 0.3724\n",
            "Epoch 30/33\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - accuracy: 0.9750 - loss: 0.0690 - val_accuracy: 0.9076 - val_loss: 0.3730\n",
            "Epoch 31/33\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - accuracy: 0.9742 - loss: 0.0684 - val_accuracy: 0.9072 - val_loss: 0.3731\n",
            "Epoch 32/33\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - accuracy: 0.9744 - loss: 0.0693 - val_accuracy: 0.9069 - val_loss: 0.3735\n",
            "Epoch 33/33\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 27ms/step - accuracy: 0.9745 - loss: 0.0677 - val_accuracy: 0.9070 - val_loss: 0.3738\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "\n",
        "print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Test loss: {test_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "benRHbHU1BFU",
        "outputId": "49150777-ae62-4f4c-9637-14f2965358c0"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9011 - loss: 0.4223\n",
            "Test accuracy: 0.9033\n",
            "Test loss: 0.4155\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "получить 0.91 не удалось\n",
        "\n",
        "максимальный результат 0.9033"
      ],
      "metadata": {
        "id": "_sFz1UDVIUJ1"
      }
    }
  ]
}